---
title: ABOUT
---

# <i class="fas fa-flask"></i> About

Welcome to the Computer Vision and Machine Learning (CVML) Lab at KAIST!  
We are a team of passionate researchers developing **robust**, **efficient**, and **trustworthy** machine learning models to address real-world challenges.

---

## Research Interests

### Generative AI
We explore advanced **text-to-image**, **video**, and **3D generation** techniques with a strong focus on diffusion models, controllable generation, and efficiency.

#### 1. Controllable & Training-free Text-to-Image Generation
- **Goal**: Enable user-guided image synthesis without model retraining.  
- **Key Works**:
  - *Scribble-Guided Diffusion for Training-free Text-to-Image Generation* — ICIP 2025  
  - *Evaluating Image Hallucination in Text-to-Image Generation with Question-Answering* — AAAI 2025

#### 2. Efficient & Memory-Aware Diffusion Models
- **Goal**: Reduce computational cost while preserving high generation quality.  
- **Key Works**:
  - *DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models* — ICLR 2025  
  - *Memory-Efficient Fine-Tuning for Quantized Diffusion Model* — ECCV 2024  
  - *Local Expert Diffusion Models for Efficient Training in Denoising Diffusion Probabilistic Models* — AAAIW 2024

#### 3. 3D Generation and Editing
- **Goal**: Achieve fast, high-quality 3D content creation with fine-grained edit control.  
- **Key Works**:
  - *DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation* — ICLR 2025

#### 4. Hallucination Mitigation in Generative Models
- **Goal**: Detect and reduce hallucinations for safer generative AI.  
- **Key Works**:
  - *Preventing Image Hallucination in Text-to-Image Generation through Factual Image Retrieval* — IJCAI TIDMwFM 2024

---

### Computer Vision under Real-World Constraints
We design robust vision algorithms for challenging environments or limited data scenarios, with applications in:
- **Autonomous driving** (e.g., LiDAR semantic segmentation)  
- **Open-vocabulary segmentation**  
- **Weakly/unsupervised learning**  
- **Efficient training** (e.g., dataset distillation)

---

### Trustworthy AI
We investigate and mitigate AI flaws by creating frameworks to evaluate and prevent:
- Hallucinations in generative models  
- Bias in AI systems

---

Our work is regularly presented at top-tier conferences such as **CVPR**, **NeurIPS**, **ICLR**, **ECCV**, **ACL**, and **AAAI**, and we maintain a collaborative, supportive research environment.
